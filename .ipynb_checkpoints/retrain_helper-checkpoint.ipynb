{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import retrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module retrain:\n",
      "\n",
      "NAME\n",
      "    retrain - Simple transfer learning with image modules from TensorFlow Hub.\n",
      "\n",
      "DESCRIPTION\n",
      "    This example shows how to train an image classifier based on any\n",
      "    TensorFlow Hub module that computes image feature vectors. By default,\n",
      "    it uses the feature vectors computed by Inception V3 trained on ImageNet.\n",
      "    See https://github.com/tensorflow/hub/blob/r0.1/docs/modules/image.md\n",
      "    for more options.\n",
      "    \n",
      "    The top layer receives as input a 2048-dimensional vector (assuming\n",
      "    Inception V3) for each image. We train a softmax layer on top of this\n",
      "    representation. If the softmax layer contains N labels, this corresponds\n",
      "    to learning N + 2048*N model parameters for the biases and weights.\n",
      "    \n",
      "    Here's an example, which assumes you have a folder containing class-named\n",
      "    subfolders, each full of images for each label. The example folder flower_photos\n",
      "    should have a structure like this:\n",
      "    \n",
      "    ~/flower_photos/daisy/photo1.jpg\n",
      "    ~/flower_photos/daisy/photo2.jpg\n",
      "    ...\n",
      "    ~/flower_photos/rose/anotherphoto77.jpg\n",
      "    ...\n",
      "    ~/flower_photos/sunflower/somepicture.jpg\n",
      "    \n",
      "    The subfolder names are important, since they define what label is applied to\n",
      "    each image, but the filenames themselves don't matter. (For a working example,\n",
      "    download http://download.tensorflow.org/example_images/flower_photos.tgz\n",
      "    and run  tar xzf flower_photos.tgz  to unpack it.)\n",
      "    \n",
      "    Once your images are prepared, and you have pip-installed tensorflow-hub and\n",
      "    a sufficiently recent version of tensorflow, you can run the training with a\n",
      "    command like this:\n",
      "    \n",
      "    ```bash\n",
      "    python retrain.py --image_dir ~/flower_photos\n",
      "    ```\n",
      "    \n",
      "    You can replace the image_dir argument with any folder containing subfolders of\n",
      "    images. The label for each image is taken from the name of the subfolder it's\n",
      "    in.\n",
      "    \n",
      "    This produces a new model file that can be loaded and run by any TensorFlow\n",
      "    program, for example the tensorflow/examples/label_image sample code.\n",
      "    \n",
      "    By default this script will use the highly accurate, but comparatively large and\n",
      "    slow Inception V3 model architecture. It's recommended that you start with this\n",
      "    to validate that you have gathered good training data, but if you want to deploy\n",
      "    on resource-limited platforms, you can try the `--tfhub_module` flag with a\n",
      "    Mobilenet model. For more information on Mobilenet, see\n",
      "    https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    Run floating-point version of Mobilenet:\n",
      "    \n",
      "    ```bash\n",
      "    python retrain.py --image_dir ~/flower_photos \\\n",
      "        --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/1\n",
      "    ```\n",
      "    \n",
      "    Run Mobilenet, instrumented for quantization:\n",
      "    \n",
      "    ```bash\n",
      "    python retrain.py --image_dir ~/flower_photos/ \\\n",
      "        --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/quantops/feature_vector/1\n",
      "    ```\n",
      "    \n",
      "    These instrumented models can be converted to fully quantized mobile models via\n",
      "    TensorFlow Lite.\n",
      "    \n",
      "    There are different Mobilenet models to choose from, with a variety of file\n",
      "    size and latency options.\n",
      "      - The first number can be '100', '075', '050', or '025' to control the number\n",
      "        of neurons (activations of hidden layers); the number of weights (and hence\n",
      "        to some extent the file size and speed) shrinks with the square of that\n",
      "        fraction.\n",
      "      - The second number is the input image size. You can choose '224', '192',\n",
      "        '160', or '128', with smaller sizes giving faster speeds.\n",
      "    \n",
      "    To use with TensorBoard:\n",
      "    \n",
      "    By default, this script will log summaries to /tmp/retrain_logs directory\n",
      "    \n",
      "    Visualize the summaries with this command:\n",
      "    \n",
      "    tensorboard --logdir /tmp/retrain_logs\n",
      "    \n",
      "    To use with Tensorflow Serving, run this tool with --saved_model_dir set\n",
      "    to some increasingly numbered export location under the model base path, e.g.:\n",
      "    \n",
      "    ```bash\n",
      "    python retrain.py (... other args as before ...) \\\n",
      "        --saved_model_dir=/tmp/saved_models/$(date +%s)/\n",
      "    tensorflow_model_server --port=9000 --model_name=my_image_classifier \\\n",
      "        --model_base_path=/tmp/saved_models/\n",
      "    ```\n",
      "\n",
      "FUNCTIONS\n",
      "    add_evaluation_step(result_tensor, ground_truth_tensor)\n",
      "        Inserts the operations we need to evaluate the accuracy of our results.\n",
      "        \n",
      "        Args:\n",
      "          result_tensor: The new final node that produces results.\n",
      "          ground_truth_tensor: The node we feed ground truth data\n",
      "          into.\n",
      "        \n",
      "        Returns:\n",
      "          Tuple of (evaluation step, prediction).\n",
      "    \n",
      "    add_final_retrain_ops(class_count, final_tensor_name, bottleneck_tensor, quantize_layer, is_training)\n",
      "        Adds a new softmax and fully-connected layer for training and eval.\n",
      "        \n",
      "        We need to retrain the top layer to identify our new classes, so this function\n",
      "        adds the right operations to the graph, along with some variables to hold the\n",
      "        weights, and then sets up all the gradients for the backward pass.\n",
      "        \n",
      "        The set up for the softmax and fully-connected layers is based on:\n",
      "        https://www.tensorflow.org/tutorials/mnist/beginners/index.html\n",
      "        \n",
      "        Args:\n",
      "          class_count: Integer of how many categories of things we're trying to\n",
      "              recognize.\n",
      "          final_tensor_name: Name string for the new final node that produces results.\n",
      "          bottleneck_tensor: The output of the main CNN graph.\n",
      "          quantize_layer: Boolean, specifying whether the newly added layer should be\n",
      "              instrumented for quantization with TF-Lite.\n",
      "          is_training: Boolean, specifying whether the newly add layer is for training\n",
      "              or eval.\n",
      "        \n",
      "        Returns:\n",
      "          The tensors for the training and cross entropy results, and tensors for the\n",
      "          bottleneck input and ground truth input.\n",
      "    \n",
      "    add_input_distortions(flip_left_right, random_crop, random_scale, random_brightness, module_spec)\n",
      "        Creates the operations to apply the specified distortions.\n",
      "        \n",
      "        During training it can help to improve the results if we run the images\n",
      "        through simple distortions like crops, scales, and flips. These reflect the\n",
      "        kind of variations we expect in the real world, and so can help train the\n",
      "        model to cope with natural data more effectively. Here we take the supplied\n",
      "        parameters and construct a network of operations to apply them to an image.\n",
      "        \n",
      "        Cropping\n",
      "        ~~~~~~~~\n",
      "        \n",
      "        Cropping is done by placing a bounding box at a random position in the full\n",
      "        image. The cropping parameter controls the size of that box relative to the\n",
      "        input image. If it's zero, then the box is the same size as the input and no\n",
      "        cropping is performed. If the value is 50%, then the crop box will be half the\n",
      "        width and height of the input. In a diagram it looks like this:\n",
      "        \n",
      "        <       width         >\n",
      "        +---------------------+\n",
      "        |                     |\n",
      "        |   width - crop%     |\n",
      "        |    <      >         |\n",
      "        |    +------+         |\n",
      "        |    |      |         |\n",
      "        |    |      |         |\n",
      "        |    |      |         |\n",
      "        |    +------+         |\n",
      "        |                     |\n",
      "        |                     |\n",
      "        +---------------------+\n",
      "        \n",
      "        Scaling\n",
      "        ~~~~~~~\n",
      "        \n",
      "        Scaling is a lot like cropping, except that the bounding box is always\n",
      "        centered and its size varies randomly within the given range. For example if\n",
      "        the scale percentage is zero, then the bounding box is the same size as the\n",
      "        input and no scaling is applied. If it's 50%, then the bounding box will be in\n",
      "        a random range between half the width and height and full size.\n",
      "        \n",
      "        Args:\n",
      "          flip_left_right: Boolean whether to randomly mirror images horizontally.\n",
      "          random_crop: Integer percentage setting the total margin used around the\n",
      "          crop box.\n",
      "          random_scale: Integer percentage of how much to vary the scale by.\n",
      "          random_brightness: Integer range to randomly multiply the pixel values by.\n",
      "          graph.\n",
      "          module_spec: The hub.ModuleSpec for the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          The jpeg input layer and the distorted result tensor.\n",
      "    \n",
      "    add_jpeg_decoding(module_spec)\n",
      "        Adds operations that perform JPEG decoding and resizing to the graph..\n",
      "        \n",
      "        Args:\n",
      "          module_spec: The hub.ModuleSpec for the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          Tensors for the node to feed JPEG data into, and the output of the\n",
      "            preprocessing steps.\n",
      "    \n",
      "    build_eval_session(module_spec, class_count)\n",
      "        Builds an restored eval session without train operations for exporting.\n",
      "        \n",
      "        Args:\n",
      "          module_spec: The hub.ModuleSpec for the image module being used.\n",
      "          class_count: Number of classes\n",
      "        \n",
      "        Returns:\n",
      "          Eval session containing the restored eval graph.\n",
      "          The bottleneck input, ground truth, eval step, and prediction tensors.\n",
      "    \n",
      "    cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor, module_name)\n",
      "        Ensures all the training, testing, and validation bottlenecks are cached.\n",
      "        \n",
      "        Because we're likely to read the same image multiple times (if there are no\n",
      "        distortions applied during training) it can speed things up a lot if we\n",
      "        calculate the bottleneck layer values once for each image during\n",
      "        preprocessing, and then just read those cached values repeatedly during\n",
      "        training. Here we go through all the images we've found, calculate those\n",
      "        values, and save them off.\n",
      "        \n",
      "        Args:\n",
      "          sess: The current active TensorFlow Session.\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          image_dir: Root folder string of the subfolders containing the training\n",
      "          images.\n",
      "          bottleneck_dir: Folder string holding cached files of bottleneck values.\n",
      "          jpeg_data_tensor: Input tensor for jpeg data from file.\n",
      "          decoded_image_tensor: The output of decoding and resizing the image.\n",
      "          resized_input_tensor: The input node of the recognition graph.\n",
      "          bottleneck_tensor: The penultimate output layer of the graph.\n",
      "          module_name: The name of the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          Nothing.\n",
      "    \n",
      "    create_bottleneck_file(bottleneck_path, image_lists, label_name, index, image_dir, category, sess, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor)\n",
      "        Create a single bottleneck file.\n",
      "    \n",
      "    create_image_lists(image_dir, testing_percentage, validation_percentage)\n",
      "        Builds a list of training images from the file system.\n",
      "        \n",
      "        Analyzes the sub folders in the image directory, splits them into stable\n",
      "        training, testing, and validation sets, and returns a data structure\n",
      "        describing the lists of images for each label and their paths.\n",
      "        \n",
      "        Args:\n",
      "          image_dir: String path to a folder containing subfolders of images.\n",
      "          testing_percentage: Integer percentage of the images to reserve for tests.\n",
      "          validation_percentage: Integer percentage of images reserved for validation.\n",
      "        \n",
      "        Returns:\n",
      "          An OrderedDict containing an entry for each label subfolder, with images\n",
      "          split into training, testing, and validation sets within each label.\n",
      "          The order of items defines the class indices.\n",
      "    \n",
      "    create_module_graph(module_spec)\n",
      "        Creates a graph and loads Hub Module into it.\n",
      "        \n",
      "        Args:\n",
      "          module_spec: the hub.ModuleSpec for the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          graph: the tf.Graph that was created.\n",
      "          bottleneck_tensor: the bottleneck values output by the module.\n",
      "          resized_input_tensor: the input images, resized as expected by the module.\n",
      "          wants_quantization: a boolean, whether the module has been instrumented\n",
      "            with fake quantization ops.\n",
      "    \n",
      "    ensure_dir_exists(dir_name)\n",
      "        Makes sure the folder exists on disk.\n",
      "        \n",
      "        Args:\n",
      "          dir_name: Path string to the folder we want to create.\n",
      "    \n",
      "    export_model(module_spec, class_count, saved_model_dir)\n",
      "        Exports model for serving.\n",
      "        \n",
      "        Args:\n",
      "          module_spec: The hub.ModuleSpec for the image module being used.\n",
      "          class_count: The number of classes.\n",
      "          saved_model_dir: Directory in which to save exported model and variables.\n",
      "    \n",
      "    get_bottleneck_path(image_lists, label_name, index, bottleneck_dir, category, module_name)\n",
      "        Returns a path to a bottleneck file for a label at the given index.\n",
      "        \n",
      "        Args:\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          label_name: Label string we want to get an image for.\n",
      "          index: Integer offset of the image we want. This will be moduloed by the\n",
      "          available number of images for the label, so it can be arbitrarily large.\n",
      "          bottleneck_dir: Folder string holding cached files of bottleneck values.\n",
      "          category: Name string of set to pull images from - training, testing, or\n",
      "          validation.\n",
      "          module_name: The name of the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          File system path string to an image that meets the requested parameters.\n",
      "    \n",
      "    get_image_path(image_lists, label_name, index, image_dir, category)\n",
      "        Returns a path to an image for a label at the given index.\n",
      "        \n",
      "        Args:\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          label_name: Label string we want to get an image for.\n",
      "          index: Int offset of the image we want. This will be moduloed by the\n",
      "          available number of images for the label, so it can be arbitrarily large.\n",
      "          image_dir: Root folder string of the subfolders containing the training\n",
      "          images.\n",
      "          category: Name string of set to pull images from - training, testing, or\n",
      "          validation.\n",
      "        \n",
      "        Returns:\n",
      "          File system path string to an image that meets the requested parameters.\n",
      "    \n",
      "    get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir, category, bottleneck_dir, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor, module_name)\n",
      "        Retrieves or calculates bottleneck values for an image.\n",
      "        \n",
      "        If a cached version of the bottleneck data exists on-disk, return that,\n",
      "        otherwise calculate the data and save it to disk for future use.\n",
      "        \n",
      "        Args:\n",
      "          sess: The current active TensorFlow Session.\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          label_name: Label string we want to get an image for.\n",
      "          index: Integer offset of the image we want. This will be modulo-ed by the\n",
      "          available number of images for the label, so it can be arbitrarily large.\n",
      "          image_dir: Root folder string of the subfolders containing the training\n",
      "          images.\n",
      "          category: Name string of which set to pull images from - training, testing,\n",
      "          or validation.\n",
      "          bottleneck_dir: Folder string holding cached files of bottleneck values.\n",
      "          jpeg_data_tensor: The tensor to feed loaded jpeg data into.\n",
      "          decoded_image_tensor: The output of decoding and resizing the image.\n",
      "          resized_input_tensor: The input node of the recognition graph.\n",
      "          bottleneck_tensor: The output tensor for the bottleneck values.\n",
      "          module_name: The name of the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          Numpy array of values produced by the bottleneck layer for the image.\n",
      "    \n",
      "    get_random_cached_bottlenecks(sess, image_lists, how_many, category, bottleneck_dir, image_dir, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor, module_name)\n",
      "        Retrieves bottleneck values for cached images.\n",
      "        \n",
      "        If no distortions are being applied, this function can retrieve the cached\n",
      "        bottleneck values directly from disk for images. It picks a random set of\n",
      "        images from the specified category.\n",
      "        \n",
      "        Args:\n",
      "          sess: Current TensorFlow Session.\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          how_many: If positive, a random sample of this size will be chosen.\n",
      "          If negative, all bottlenecks will be retrieved.\n",
      "          category: Name string of which set to pull from - training, testing, or\n",
      "          validation.\n",
      "          bottleneck_dir: Folder string holding cached files of bottleneck values.\n",
      "          image_dir: Root folder string of the subfolders containing the training\n",
      "          images.\n",
      "          jpeg_data_tensor: The layer to feed jpeg image data into.\n",
      "          decoded_image_tensor: The output of decoding and resizing the image.\n",
      "          resized_input_tensor: The input node of the recognition graph.\n",
      "          bottleneck_tensor: The bottleneck output layer of the CNN graph.\n",
      "          module_name: The name of the image module being used.\n",
      "        \n",
      "        Returns:\n",
      "          List of bottleneck arrays, their corresponding ground truths, and the\n",
      "          relevant filenames.\n",
      "    \n",
      "    get_random_distorted_bottlenecks(sess, image_lists, how_many, category, image_dir, input_jpeg_tensor, distorted_image, resized_input_tensor, bottleneck_tensor)\n",
      "        Retrieves bottleneck values for training images, after distortions.\n",
      "        \n",
      "        If we're training with distortions like crops, scales, or flips, we have to\n",
      "        recalculate the full model for every image, and so we can't use cached\n",
      "        bottleneck values. Instead we find random images for the requested category,\n",
      "        run them through the distortion graph, and then the full graph to get the\n",
      "        bottleneck results for each.\n",
      "        \n",
      "        Args:\n",
      "          sess: Current TensorFlow Session.\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          how_many: The integer number of bottleneck values to return.\n",
      "          category: Name string of which set of images to fetch - training, testing,\n",
      "          or validation.\n",
      "          image_dir: Root folder string of the subfolders containing the training\n",
      "          images.\n",
      "          input_jpeg_tensor: The input layer we feed the image data to.\n",
      "          distorted_image: The output node of the distortion graph.\n",
      "          resized_input_tensor: The input node of the recognition graph.\n",
      "          bottleneck_tensor: The bottleneck output layer of the CNN graph.\n",
      "        \n",
      "        Returns:\n",
      "          List of bottleneck arrays and their corresponding ground truths.\n",
      "    \n",
      "    main(_)\n",
      "    \n",
      "    prepare_file_system()\n",
      "    \n",
      "    run_bottleneck_on_image(sess, image_data, image_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor)\n",
      "        Runs inference on an image to extract the 'bottleneck' summary layer.\n",
      "        \n",
      "        Args:\n",
      "          sess: Current active TensorFlow Session.\n",
      "          image_data: String of raw JPEG data.\n",
      "          image_data_tensor: Input data layer in the graph.\n",
      "          decoded_image_tensor: Output of initial image resizing and preprocessing.\n",
      "          resized_input_tensor: The input node of the recognition graph.\n",
      "          bottleneck_tensor: Layer before the final softmax.\n",
      "        \n",
      "        Returns:\n",
      "          Numpy array of bottleneck values.\n",
      "    \n",
      "    run_final_eval(train_session, module_spec, class_count, image_lists, jpeg_data_tensor, decoded_image_tensor, resized_image_tensor, bottleneck_tensor)\n",
      "        Runs a final evaluation on an eval graph using the test data set.\n",
      "        \n",
      "        Args:\n",
      "          train_session: Session for the train graph with the tensors below.\n",
      "          module_spec: The hub.ModuleSpec for the image module being used.\n",
      "          class_count: Number of classes\n",
      "          image_lists: OrderedDict of training images for each label.\n",
      "          jpeg_data_tensor: The layer to feed jpeg image data into.\n",
      "          decoded_image_tensor: The output of decoding and resizing the image.\n",
      "          resized_image_tensor: The input node of the recognition graph.\n",
      "          bottleneck_tensor: The bottleneck output layer of the CNN graph.\n",
      "    \n",
      "    save_graph_to_file(graph, graph_file_name, module_spec, class_count)\n",
      "        Saves an graph to file, creating a valid quantized one if necessary.\n",
      "    \n",
      "    should_distort_images(flip_left_right, random_crop, random_scale, random_brightness)\n",
      "        Whether any distortions are enabled, from the input flags.\n",
      "        \n",
      "        Args:\n",
      "          flip_left_right: Boolean whether to randomly mirror images horizontally.\n",
      "          random_crop: Integer percentage setting the total margin used around the\n",
      "          crop box.\n",
      "          random_scale: Integer percentage of how much to vary the scale by.\n",
      "          random_brightness: Integer range to randomly multiply the pixel values by.\n",
      "        \n",
      "        Returns:\n",
      "          Boolean value indicating whether any distortions should be applied.\n",
      "    \n",
      "    variable_summaries(var)\n",
      "        Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
      "\n",
      "DATA\n",
      "    CHECKPOINT_NAME = '/tmp/_retrain_checkpoint'\n",
      "    FAKE_QUANT_OPS = ('FakeQuantWithMinMaxVars', 'FakeQuantWithMinMaxVarsP...\n",
      "    FLAGS = None\n",
      "    MAX_NUM_IMAGES_PER_CLASS = 134217727\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\jhwel\\documents\\python_projects\\phys_research\\cancer_cells\\retrain.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  help(retrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Hub modules : https://github.com/tensorflow/hub/blob/r0.1/docs/modules/image.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v1 = 'https://tfhub.dev/google/imagenet/inception_v1/classification/1'\n",
    "inception_v2 = 'https://tfhub.dev/google/imagenet/inception_v2/classification/1'\n",
    "inception_v3 = 'https://tfhub.dev/google/imagenet/inception_v3/classification/1'\n",
    "inception_resnet_v2 = 'https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/1'\n",
    "mobilenet_v1 = 'https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/1'\n",
    "mobilenet_v2 = 'https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'cleaned_cell_type_images'  # Path to folders of labeled images.\n",
    "output_graph =  'output_files/output_graph.pb' # Where to save the trained graph.\n",
    "output_labels =  'output_files/output_labels.txt' #'Where to save the trained graph\\'s labels.'\n",
    "summaries_dir =  'output_files/retrain_logs' #'Where to save summary logs for TensorBoard.'\n",
    "how_many_training_steps = '4000' # default 4000\n",
    "learning_rate = '0.01'  # default 0.01\n",
    "train_batch_size = '100'  # default 100\n",
    "\n",
    "print_misclassified_test_images = 'True' # Whether to print out a list of all misclassified test images.\n",
    "tfhub_module = inception_v3 \n",
    "saved_model_dir = 'output_files/saved_model' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir0 = ' --image_dir ' + image_dir\n",
    "output_graph0 = ' --output_graph ' + output_graph\n",
    "output_labels0 = ' --output_labels ' + output_labels\n",
    "summaries_dir0 = ' --summaries_dir ' +  summaries_dir\n",
    "how_many_training_steps0 = ' --how_many_training_steps ' + how_many_training_steps\n",
    "learning_rate0 = ' --learning_rate ' + learning_rate\n",
    "train_batch_size0 = ' --train_batch_size ' + train_batch_size\n",
    "print_misclassified_test_images0 = ' --print_misclassified_test_images ' + print_misclassified_test_images\n",
    "tfhub_module0 = ' --tfhub_module ' + tfhub_module\n",
    "saved_model_dir0 = ' --saved_model_dir ' + saved_model_dir\n",
    "\n",
    "subprocess.call('python retrain.py'\n",
    "                + image_dir0  \n",
    "                + output_graph0\n",
    "                + output_labels0\n",
    "                + summaries_dir0\n",
    "                + how_many_training_steps0\n",
    "                + learning_rate0\n",
    "                + train_batch_size0\n",
    "                + print_misclassified_test_images0\n",
    "                + tfhub_module0\n",
    "                + saved_model_dir0, \n",
    "                shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
